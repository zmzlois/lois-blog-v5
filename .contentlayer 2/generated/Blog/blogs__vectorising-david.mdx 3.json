{
  "title": "Vectorising David",
  "date": "2023-01-07T00:00:00.000Z",
  "published": true,
  "featured": true,
  "description": "Part of the original draft I have written for the book Prompt",
  "author": "zmzlois",
  "body": {
    "raw": "\n\n##### This article is part of the original draft I have written for the book [Prompt](https://prompt.mba/) with my friend [David Boyle](https://medium.com/u/85197d987a5b?source=post_page-----48253fc1582--------------------------------). Touch lightly on transformer architecture and how tokeniser work.\n\n12 min read\n\n**A very brief intro in case the whole article is too long to read:**\n\nThink about words in Bytes, a bunch of 0 and 1. If you get the impression that AI is all about probabilities–you are not wrong. ChatGPT uses a bunch of 0 and 1, encoded from words within sentences, as input and a huge vocabulary dictionary. It didn’t generate a whole block of text for you; it was “thinking” the best next word while generating every word. In a nutshell, this is how GPT-like models output in the simplest form:\n\n- _This (First words and then what’s the next most possible word?)_\n- _This book (High probability as it is closer to the prompt)_\n- _This book talks (High probability a verb appears after a noun)_\n- _This book talks about_\n- _This book talks about artificial_\n- _This book talks about artificial intelligence_\n- _This book talks about artificial intelligence in_\n- _This book talks about artificial intelligence in marketing_\n- _This book talks about artificial intelligence in marketing._\n\nEvery time it generates a word, it looks for the next most possible word to pair with the previous one. OpenAI hired 40 data trainers / human labellers to generate training data and give feedback on the results. Then it iterates this process with more prompts, data, and feedback.\n\n### Context: Beyond the pattern of a sentence\n\n\nOne of the most common natural language processing applications is at our fingertips: every time we type in something on our phone it suggests the next word.\n\nIn earlier NLP models, say 2003, [it treats words with static value](https://ecommons.cornell.edu/bitstream/handle/1813/6721/87-881.pdf;jsessionid=CE51FD6F7E2819ED24988A424EEA725E?sequence=1)s. The context of “south” within “Something went south.” and “I live in the south bank.” has no difference. People created [a rule-based structure–using cues](https://arxiv.org/pdf/1706.03762.pdf), such as negation to help NLP models perform better in linguistic tasks in 2012. It was quickly revoked because it draws technical limitations — how big can this cue dictionary go? Based on the technical limitation we can already infer that the model won’t be able to understand context, nor achieve more complex linguistic tasks (ex: your prompt has several lines of text and include conflicting cues).\n\n![Example of false negation cues](https://miro.medium.com/v2/resize:fit:1062/0*fHi9-KPrYyixzbvw)\n_Source: Example of false negation cues from UCM-I: A Rule-based Syntactic Approach for Resolving the Scope of Negation_\n\n## **Context is important.**\n\nTake an example if someone posted on Twitter at 7:35 am:\n\n“ Thanks for coming in today, coach.”\n\nIt sounded like sheer appreciation. But if he also tweeted at 6:09 am saying,\n\n“Omg it’s snowing right now, and our class is supposed to start at 6 am! The coach is almost 10 minutes late!!! How can she just let us stand in the snow and wait? Shall we just leave like the teacher didn’t show up?!”\n\nThen we have an idea that he is probably being sarcastic about the coach being late. But breaking the two sentences separately, we have very little idea about what he was referring to or what it actually means. We will return to this “context” problem a bit because we have another problem before going into some state-of-the-art neural network. And to be very honest, any neural network is just matrix multiplication. We tried to write them in plain English. Secondary school level maths is enough. If you are not happy, we have also linked to multiple resources so you can check them up — as always :)\n\nMatrices are rows of vectors(or numbers, but here we refer to vectors because they have to represent length and position later on). And matrix multiplications are rows of vectors multiplied by rows of vectors. Vectors are objects with certain properties. It has both a magnitude and direction.\n\n**Actually, we have a big Problem: Computers can’t understand text but numbers, so how are languages being processed?**\n\n### Tokenizers: Translator between machine and human\n\n\n\nA high-performance, high-functional tokenizer is essential in a machine-learning system focusing on NLP tasks. Because your users don’t want to wait as soon as they hit enter. In production, time is calculated in milliseconds. The longer they wait, the more likely they will leave the site with bad reviews. And that happened on a lot of generative AI applications.\n\nConverting words into plain binary form is not enough. The objective of a tokenizer is simple: find the most meaningful, relevant way to represent texts by numbers. It splits text into words or subwords; then words are converted to ids through a look-up table. And there are multiple tokenization algorithms. The sentence “David decided to write a book.” can be converted to\n\n\\[“David”, “decided”, “to”, “write”, “ a”, “book”, “.”\\] to get\n\n\\[(“David”,15), (“decided”20), (“to”, 5), (“write”,10), (“ a”,3), ( “book”,8), ( “.”, 1)\\]\n\nor character-based like \\[“D”, “a”, “v”,…. “o”, “k”, “.”\\].\n\nThe former requires a huge vocabulary bank, a lot of memory and time to consume large text. Character-based tokenization is relatively easy and saves a lot of memory(less vocabulary!) and time. Still, it is harder for the model to learn the actual pattern of a language. In production and actual training, it results in serious performance loss.\n\n**Take an example: Say you make a search engine and train the model with a character-based method, and you set a fuzzy search where that anything that matches 30% would count towards a related search result. I went on to search “ChatGPT Prompt” and on top of my search results could be “Progressive Web App” and “Chat with Putin”. Earlier NLP models used it, and the result was a joke.**\n\nTo improve model performance, models nowadays use a hybrid approach between word-level and character-level methods called subword tokenization. There are several of them; BERT uses [WordPiece](https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf) and ChatGPT uses Byte-level [Byte-Pair Encoding](https://arxiv.org/pdf/1508.07909.pdf). The wording is irrelevant; think about how you refactor a function like this:\n\n24x⁴ + 8x³ + 12x² = y → Refactored → 4x²(6x²+2x+3) = y\n\n\\[“David found his id card on top of his wounds.”\\] will be broken down into\n\n\\[“Dav”, “id”, “f”, “ound”, “his”, “id”, “card”, “on”, “top”, “of”, “his”, “w”, “ound”, “s”.\\]\n\nMathematically they are similar. Byte-level Byte-Pair Encoding is a very clever method in all the tokenization algorithms(they are usually pretrained too). It refactored the most frequent character pair and provided the most frequent pair with relevant values. It also forces the base vocabulary to be 256 with additional rules to deal with punctuation/spacing in a sentence. Within the vocabulary of 50,257, each corresponds to the [256-byte base tokens, a special end-of-text token and the symbols learned with 50,000 merges](https://huggingface.co/docs/transformers/tokenizer_summary#bytelevel-bpe). You can try to “tokenize” something yourself at [OpenAI’s tokenizing tool](https://beta.openai.com/tokenizer).\n\nNow the gap between humans and machines is sorted. How shall we solve the context problem?\n\n### Attention is all you need: The science of context and transformer architecture\n\nResearchers were scratching their heads to find what could be the best solution for the next generation neural network — a model doesn’t just do a dummy job–prompting a simple sentence and outputting something silly; something avoids [vanishing or exploding gradient problems](https://programmathically.com/understanding-the-exploding-and-vanishing-gradients-problem/#:~:text=The%20exploding%20gradient%20problem%20describes,weights%2C%20and%20learning%20becomes%20unstable.); something can take a sequence as input and give a sequence as output; something has longer memory; a model saves computation cost and able to be trained with a relatively smaller amount of text.\n\nBasically, they were looking for god.\n\n[Bag-of-words](https://en.wikipedia.org/wiki/Bag-of-words_model), gone. [Recurrent neural networks](http://d2l.ai/chapter_recurrent-neural-networks/rnn.html), gone. [Long Short-Term Memory](https://d2l.ai/chapter_recurrent-modern/lstm.html), gone. For all the reasons listed above.\n\nTo address the context problem and issues that occurred in other models, a bunch of researchers from Google Brain (some of them work in OpenAI now!) dropped a bombing article, [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf) in 2017, which is still at the forefront of any headlines of NLP models that shatters performance benchmarks. It brought a model that worked like a human brain(close to, but not enough) and accidentally changed computer vision, image generation and audio processing. But the strongest use cases were still in NLP: [GPT](https://huggingface.co/docs/transformers/main/en/model_doc/openai-gpt) and [BERT](https://huggingface.co/docs/transformers/main/en/model_doc/bert). Both have a lot of variations in handling different tasks, and the model proposed in the original paper has evolved since.\n\n## **Human vision systems inspired the attention mechanism proposed in the article.**\n\nImagine how your brain processes when you see something. Say you walk into a restaurant during Christmas; you’d pay attention to the most stunning element, the Christmas Tree, the pretty waitress holding the menu waiting for you to take you to the seat, and then the seats scattered around the restaurant. You’d ignore insignificant details like the material of the floor, other customers in the restaurant or dimmed lights at the ceiling because they came off as irrelevant.\n\nSame as the attention mechanism. It focuses on the most relevant parts in a whole block of text and anything relevant but ignores the insignificant details. Supporting the attention mechanism, they introduced a neural network called the Transformer which came with an **encoder-decoder architecture**. In a nutshell, an encoder is the part that deals with input pre-processing, and the decoder deals with output pre-processing.\n\n![Illustrated guide to transformers](https://miro.medium.com/v2/resize:fit:1116/0*lIpCoChAI8dW2hzj)\n_Source: Michael Phi, Illustrated Guide to transformers step by step explanation_\n\nLike every data science project, there is a lot of preprocessing before the actual work.\n\n### Preprocessing 🤯\n\n\nThe first step is input encoding. We have mentioned tokenizers before. It will convert “David” into numbers first (I am using one-hot encoding here because it makes much sense since you are aware computers work with binaries, but it is [a bad choice](https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#one-hot-vectors-are-a-bad-choice) if you come across someone using it. ) at a vector size of 50,257.\n\n![Input encoding](https://miro.medium.com/v2/resize:fit:1400/0*JNOmd5_Q0DM14PlT)\n_Figure 29. Input encoding_\n\nGPT’s input sequence is [defaulted to 2,048 words](https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/gpt2#transformers.GPT2Config.n_positions). And yeah we do that with every word. GPT-like models generally take the input and guess the next best possible output.\n\n![Input and output sequences](https://miro.medium.com/v2/resize:fit:1400/0*bVbzK70HIebcALus)\n_Input and Output sequences_\n\nAnd both input and output default to 2,048 words.\n\n![2,048 words](https://miro.medium.com/v2/resize:fit:1400/0*hbeQKs9DlEBV_sFr)\n\nSo we have a matrix of 2,048 x 50,257 and end up with a bunch of zeros and ones.\n\n![Matrix encoding](https://miro.medium.com/v2/resize:fit:1400/0*ZmwOUg5yR4lY3ouy)\n_Matrix encoding_\n\nWait a minute, GPT-3 uses Byte-level BPE, so the output will look like the below before it was cut down to zero and ones.\n\n\\[11006, 3066, 284, 3551, 257\\]\n\nMake something shorter. Something not 50,257. Also this is a really short sentence, so where are the rest of the 2,043 words? They are filled in with empty values, and the encoder is told to ignore them (a process called **masking**; otherwise the empty value will influence the whole sentence and the model can’t pay attention to important things).\n\nBut there are still many vectors to deal with for longer sentences, and we know they force the base vocabulary to be zero and ones. So there are a lot of empty spaces and wasted storage.\n\nTo solve this, we use an **embedding function**: a function takes the long vector of zeros and ones and then outputs an n-length vector of numbers like so. ([Word Embedding](http://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#word-embedding-word2vec), the process of mapping words to real vectors)\n\n![Embedding function in action](https://miro.medium.com/v2/resize:fit:692/0*Lhb25rqsunk0DDyA)\n_An embedding function in action_\n\nAnd then store them in a smaller dimension space (if the vector length is 2, it will be stored in a 2-dimensional space like x-y axis ones)\n\n![Words in a dimension space](https://miro.medium.com/v2/resize:fit:940/0*2rczZuh9wYM2-qiY)\n_Words in a dimension space_\n\nBut yeah, GPT’s dimension is much larger than two after multiplication between matrices: 12288 dimensions. ( A lot larger than the original paper which only has 512)\n\nThe first matrix below is a 2,048 x 50,257 sequence-encoding matrix.\n\nThe second matrix is a 50,257 x 122,88 embedding-weight matrix(learnt by model).\n\n![Matrix multiplication](https://miro.medium.com/v2/resize:fit:1400/0*i9a-QMGxmOX7Xg0T)\n_Figure 35. Matrix multiplication_\n\n[Multiply both of them](https://www.mathsisfun.com/algebra/matrix-multiplying.html) we get a 2,048 x 12,288 sequence-embedding matrix, and every word is represented by numbers on a “look-up table” for the model to look at. Awesome, everything is represented by numbers now. But so far we have no information on the absolute or relative position or the words(or token if you might call it).\n\nSo we do some [**positional encoding**](https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html#positional-encoding) to give every word a position id to notify the model of each word’s position in the text before going into the multi-head attention layer (check the link if you are interested in the related literature, maths and python code).\n\n## Encoder and Decoder\n\n\n![Encoder and decoder](https://miro.medium.com/v2/resize:fit:1116/0*z8iroxIM5cQJYz3V)\n_Encoder and Decoder_\n\nOh geez, that was a hell lot of preprocessing; it’s time to get some exciting work done. You can crunch on this baby graph or go back to 4 pages before and look at the bigger transformer architecture picture. [What do they do](https://jalammar.github.io/illustrated-transformer/)? The encoder on the left maps an input sequence of text representations to a sequence of continuous representations.\n\nThe decoder on the right generated an output sequence of symbols one element at a time. At each step the model is auto-regressive and consumes the previously generated text as additional input when generating the next. Like what we said before (Imagine the turtle in Zootopia spilling out words one by one, but much faster)And the transformer allows this overall architecture using **stacked self-attention**, **point-wise, fully connected layers** to both the encoder and decoder. To clear some terms in case you get confused.\n\n[**Stacked self-attention**](https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d): A self-attention module takes x input and x output, is the mechanism to allow the input to interact with each other(“self”) and find out who they should pay more attention to (“attention”) and aggregates these interactions and attention scores as output. Stacked simply means: multiple times/layers.\n\nTo clear out how this works. Consider a sentence: David decided to write a book about ChatGPT because it is revolutionary.\n\n![Attention head](https://miro.medium.com/v2/resize:fit:1400/0*up1JcpKkKny8s1NW)\n_Attention head_\n\nTransformer doubled down the idea of attention and packed together individual attention elements known as the “**attention head**” to “focus” on a word. It can tell the model how relevant or important that word is to understand the current word being parsed. Like the figure above we “focused” on \\[“write”\\] and its relationship with others.\n\nThe model learns three linear projections when the text goes through it(some [trigonometry functions](https://mathworld.wolfram.com/Trigonometry.html#:~:text=The%20study%20of%20angles%20and,secant%20%2C%20sine%20%2C%20and%20tangent%20.)): “Queries(denoted as Q)”, “Keys(denoted as K)” and “Values(denoted as V)” and GPT-3 repeat this process for 96 times to create multi-head attention, each with a different learnt query, key, value projection weights.\n\n![Attention function](https://miro.medium.com/v2/resize:fit:1400/0*z3U4zDbPZj0HLaS6)\n_Attention function_\n\nMore attention head(**multi-head attention**) in one layer means the model can look back or forward in long text.\n\nOur sample text was too short to have a beautiful graph so I grabbed this to help you see how beautiful it is with multi-head attention:\n\n![multi-head](https://miro.medium.com/v2/resize:fit:1400/0*MGq43AQvoSo1U8M8)\nSource: [Review — Attention is all you need](https://sh-tsang.medium.com/review-attention-is-all-you-need-transformer-96c787ecdec1)\n\nAnd the results of each attention head are concatenated together yielding a 2048x12288 matrix, which is then multiplied with a linear projection (which doesn’t change the matrix shape).\n\n![Ensuring matrix shape doesn't change](https://miro.medium.com/v2/resize:fit:1400/0*OAIle3NBiptXTcb5)\n\n[**Point-wise fully connected layer**](https://paperswithcode.com/method/position-wise-feed-forward-layer): Means all connections between neurons(they are just numbers, in this case, there are several ones behind the dot like 0.00000012) have a fixed weight of 1(they all add up to 1) which means the output of each neuron in the previous layer is passed through to the next layer without any modification. This method allows the model to improve performance(without neglecting words).\n\nAnd more layers of attention mean that your model can then learn a higher level of both syntactic structures and potentially, semantic meaning. The table below shows a dummy example of what the output for the word “write” from the attention layer might look like by multiplying different weight matrices together and each of them obtaining a weight to try and identify which words in the sentence the network should “pay attention” if they are important in the context.\n\n![Weights of a sentence](https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_SLvMH3wuKO-iuRdenJpw.png)\n\nFinally we tackled the context problem. BERT-like models and GPT-like models both use the attention mechanism of the transformer architecture to learn context from text.\n\nAnd by learning context, potentially, these models can develop some level of language skill (how they function is closer to the human brain after all! Note that I used “closer”, not “close”. We still have a long way to go until we reach a place where models I’m work like the brain.) which enables them to perform better on a range of language tasks(answering questions, summarising etc.).\n\nAfter every output we get a matrix of 2,048x12,288! Huge! Now we just need to reverse engineer the [work embedding](http://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#word-embedding-word2vec) process and map the data back to text. After all, we spent a hell lot of time learning it!! Also GPT uses the parameter top-k to limit the output, so it always picks the most likely words to make it grammatically correct — seeing something not true? Don’t get surprised.\n\nAnother thing: GPT-3, the base model of ChatGPT, uses [sparse attention](https://openai.com/blog/sparse-transformer/), allowing more efficient computation. So you see, these guys designed the whole architecture, everything with speed in mind.\n\nThat was long 🫠 And I still have a load in reinforcement learning with human feedback left in my draft.\n\nUntil next time 🫡\n",
    "code": "var Component=(()=>{var dn=Object.create;var A=Object.defineProperty;var un=Object.getOwnPropertyDescriptor;var bn=Object.getOwnPropertyNames;var cn=Object.getPrototypeOf,mn=Object.prototype.hasOwnProperty;var V=(u,n)=>()=>(n||u((n={exports:{}}).exports,n),n.exports),fn=(u,n)=>{for(var g in n)A(u,g,{get:n[g],enumerable:!0})},xe=(u,n,g,v)=>{if(n&&typeof n==\"object\"||typeof n==\"function\")for(let N of bn(n))!mn.call(u,N)&&N!==g&&A(u,N,{get:()=>n[N],enumerable:!(v=un(n,N))||v.enumerable});return u};var hn=(u,n,g)=>(g=u!=null?dn(cn(u)):{},xe(n||!u||!u.__esModule?A(g,\"default\",{value:u,enumerable:!0}):g,u)),gn=u=>xe(A({},\"__esModule\",{value:!0}),u);var we=V((yn,ye)=>{ye.exports=React});var De=V(q=>{\"use strict\";(function(){\"use strict\";var u=we(),n=Symbol.for(\"react.element\"),g=Symbol.for(\"react.portal\"),v=Symbol.for(\"react.fragment\"),N=Symbol.for(\"react.strict_mode\"),$=Symbol.for(\"react.profiler\"),K=Symbol.for(\"react.provider\"),X=Symbol.for(\"react.context\"),H=Symbol.for(\"react.forward_ref\"),S=Symbol.for(\"react.suspense\"),O=Symbol.for(\"react.suspense_list\"),k=Symbol.for(\"react.memo\"),I=Symbol.for(\"react.lazy\"),ke=Symbol.for(\"react.offscreen\"),Q=Symbol.iterator,Te=\"@@iterator\";function Ee(e){if(e===null||typeof e!=\"object\")return null;var i=Q&&e[Q]||e[Te];return typeof i==\"function\"?i:null}var y=u.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED;function m(e){{for(var i=arguments.length,o=new Array(i>1?i-1:0),r=1;r<i;r++)o[r-1]=arguments[r];Pe(\"error\",e,o)}}function Pe(e,i,o){{var r=y.ReactDebugCurrentFrame,a=r.getStackAddendum();a!==\"\"&&(i+=\"%s\",o=o.concat([a]));var d=o.map(function(s){return String(s)});d.unshift(\"Warning: \"+i),Function.prototype.apply.call(console[e],console,d)}}var Re=!1,Ce=!1,Ae=!1,Se=!1,Oe=!1,J;J=Symbol.for(\"react.module.reference\");function Ie(e){return!!(typeof e==\"string\"||typeof e==\"function\"||e===v||e===$||Oe||e===N||e===S||e===O||Se||e===ke||Re||Ce||Ae||typeof e==\"object\"&&e!==null&&(e.$$typeof===I||e.$$typeof===k||e.$$typeof===K||e.$$typeof===X||e.$$typeof===H||e.$$typeof===J||e.getModuleId!==void 0))}function je(e,i,o){var r=e.displayName;if(r)return r;var a=i.displayName||i.name||\"\";return a!==\"\"?o+\"(\"+a+\")\":o}function Z(e){return e.displayName||\"Context\"}function _(e){if(e==null)return null;if(typeof e.tag==\"number\"&&m(\"Received an unexpected object in getComponentNameFromType(). This is likely a bug in React. Please file an issue.\"),typeof e==\"function\")return e.displayName||e.name||null;if(typeof e==\"string\")return e;switch(e){case v:return\"Fragment\";case g:return\"Portal\";case $:return\"Profiler\";case N:return\"StrictMode\";case S:return\"Suspense\";case O:return\"SuspenseList\"}if(typeof e==\"object\")switch(e.$$typeof){case X:var i=e;return Z(i)+\".Consumer\";case K:var o=e;return Z(o._context)+\".Provider\";case H:return je(e,e.render,\"ForwardRef\");case k:var r=e.displayName||null;return r!==null?r:_(e.type)||\"Memo\";case I:{var a=e,d=a._payload,s=a._init;try{return _(s(d))}catch{return null}}}return null}var x=Object.assign,G=0,ee,ne,te,ie,oe,re,le;function se(){}se.__reactDisabledLog=!0;function ze(){{if(G===0){ee=console.log,ne=console.info,te=console.warn,ie=console.error,oe=console.group,re=console.groupCollapsed,le=console.groupEnd;var e={configurable:!0,enumerable:!0,value:se,writable:!0};Object.defineProperties(console,{info:e,log:e,warn:e,error:e,group:e,groupCollapsed:e,groupEnd:e})}G++}}function Fe(){{if(G--,G===0){var e={configurable:!0,enumerable:!0,writable:!0};Object.defineProperties(console,{log:x({},e,{value:ee}),info:x({},e,{value:ne}),warn:x({},e,{value:te}),error:x({},e,{value:ie}),group:x({},e,{value:oe}),groupCollapsed:x({},e,{value:re}),groupEnd:x({},e,{value:le})})}G<0&&m(\"disabledDepth fell below zero. This is a bug in React. Please file an issue.\")}}var j=y.ReactCurrentDispatcher,z;function T(e,i,o){{if(z===void 0)try{throw Error()}catch(a){var r=a.stack.trim().match(/\\n( *(at )?)/);z=r&&r[1]||\"\"}return`\n`+z+e}}var F=!1,E;{var Me=typeof WeakMap==\"function\"?WeakMap:Map;E=new Me}function ae(e,i){if(!e||F)return\"\";{var o=E.get(e);if(o!==void 0)return o}var r;F=!0;var a=Error.prepareStackTrace;Error.prepareStackTrace=void 0;var d;d=j.current,j.current=null,ze();try{if(i){var s=function(){throw Error()};if(Object.defineProperty(s.prototype,\"props\",{set:function(){throw Error()}}),typeof Reflect==\"object\"&&Reflect.construct){try{Reflect.construct(s,[])}catch(p){r=p}Reflect.construct(e,[],s)}else{try{s.call()}catch(p){r=p}e.call(s.prototype)}}else{try{throw Error()}catch(p){r=p}e()}}catch(p){if(p&&r&&typeof p.stack==\"string\"){for(var l=p.stack.split(`\n`),f=r.stack.split(`\n`),b=l.length-1,c=f.length-1;b>=1&&c>=0&&l[b]!==f[c];)c--;for(;b>=1&&c>=0;b--,c--)if(l[b]!==f[c]){if(b!==1||c!==1)do if(b--,c--,c<0||l[b]!==f[c]){var h=`\n`+l[b].replace(\" at new \",\" at \");return e.displayName&&h.includes(\"<anonymous>\")&&(h=h.replace(\"<anonymous>\",e.displayName)),typeof e==\"function\"&&E.set(e,h),h}while(b>=1&&c>=0);break}}}finally{F=!1,j.current=d,Fe(),Error.prepareStackTrace=a}var D=e?e.displayName||e.name:\"\",ve=D?T(D):\"\";return typeof e==\"function\"&&E.set(e,ve),ve}function Be(e,i,o){return ae(e,!1)}function We(e){var i=e.prototype;return!!(i&&i.isReactComponent)}function P(e,i,o){if(e==null)return\"\";if(typeof e==\"function\")return ae(e,We(e));if(typeof e==\"string\")return T(e);switch(e){case S:return T(\"Suspense\");case O:return T(\"SuspenseList\")}if(typeof e==\"object\")switch(e.$$typeof){case H:return Be(e.render);case k:return P(e.type,i,o);case I:{var r=e,a=r._payload,d=r._init;try{return P(d(a),i,o)}catch{}}}return\"\"}var R=Object.prototype.hasOwnProperty,de={},ue=y.ReactDebugCurrentFrame;function C(e){if(e){var i=e._owner,o=P(e.type,e._source,i?i.type:null);ue.setExtraStackFrame(o)}else ue.setExtraStackFrame(null)}function Le(e,i,o,r,a){{var d=Function.call.bind(R);for(var s in e)if(d(e,s)){var l=void 0;try{if(typeof e[s]!=\"function\"){var f=Error((r||\"React class\")+\": \"+o+\" type `\"+s+\"` is invalid; it must be a function, usually from the `prop-types` package, but received `\"+typeof e[s]+\"`.This often happens because of typos such as `PropTypes.function` instead of `PropTypes.func`.\");throw f.name=\"Invariant Violation\",f}l=e[s](i,s,r,o,null,\"SECRET_DO_NOT_PASS_THIS_OR_YOU_WILL_BE_FIRED\")}catch(b){l=b}l&&!(l instanceof Error)&&(C(a),m(\"%s: type specification of %s `%s` is invalid; the type checker function must return `null` or an `Error` but returned a %s. You may have forgotten to pass an argument to the type checker creator (arrayOf, instanceOf, objectOf, oneOf, oneOfType, and shape all require an argument).\",r||\"React class\",o,s,typeof l),C(null)),l instanceof Error&&!(l.message in de)&&(de[l.message]=!0,C(a),m(\"Failed %s type: %s\",o,l.message),C(null))}}}var Ye=Array.isArray;function M(e){return Ye(e)}function Ve(e){{var i=typeof Symbol==\"function\"&&Symbol.toStringTag,o=i&&e[Symbol.toStringTag]||e.constructor.name||\"Object\";return o}}function qe(e){try{return be(e),!1}catch{return!0}}function be(e){return\"\"+e}function ce(e){if(qe(e))return m(\"The provided key is an unsupported type %s. This value must be coerced to a string before before using it here.\",Ve(e)),be(e)}var U=y.ReactCurrentOwner,$e={key:!0,ref:!0,__self:!0,__source:!0},me,fe,B;B={};function Ke(e){if(R.call(e,\"ref\")){var i=Object.getOwnPropertyDescriptor(e,\"ref\").get;if(i&&i.isReactWarning)return!1}return e.ref!==void 0}function Xe(e){if(R.call(e,\"key\")){var i=Object.getOwnPropertyDescriptor(e,\"key\").get;if(i&&i.isReactWarning)return!1}return e.key!==void 0}function Qe(e,i){if(typeof e.ref==\"string\"&&U.current&&i&&U.current.stateNode!==i){var o=_(U.current.type);B[o]||(m('Component \"%s\" contains the string ref \"%s\". Support for string refs will be removed in a future major release. This case cannot be automatically converted to an arrow function. We ask you to manually fix this case by using useRef() or createRef() instead. Learn more about using refs safely here: https://reactjs.org/link/strict-mode-string-ref',_(U.current.type),e.ref),B[o]=!0)}}function Je(e,i){{var o=function(){me||(me=!0,m(\"%s: `key` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",i))};o.isReactWarning=!0,Object.defineProperty(e,\"key\",{get:o,configurable:!0})}}function Ze(e,i){{var o=function(){fe||(fe=!0,m(\"%s: `ref` is not a prop. Trying to access it will result in `undefined` being returned. If you need to access the same value within the child component, you should pass it as a different prop. (https://reactjs.org/link/special-props)\",i))};o.isReactWarning=!0,Object.defineProperty(e,\"ref\",{get:o,configurable:!0})}}var en=function(e,i,o,r,a,d,s){var l={$$typeof:n,type:e,key:i,ref:o,props:s,_owner:d};return l._store={},Object.defineProperty(l._store,\"validated\",{configurable:!1,enumerable:!1,writable:!0,value:!1}),Object.defineProperty(l,\"_self\",{configurable:!1,enumerable:!1,writable:!1,value:r}),Object.defineProperty(l,\"_source\",{configurable:!1,enumerable:!1,writable:!1,value:a}),Object.freeze&&(Object.freeze(l.props),Object.freeze(l)),l};function nn(e,i,o,r,a){{var d,s={},l=null,f=null;o!==void 0&&(ce(o),l=\"\"+o),Xe(i)&&(ce(i.key),l=\"\"+i.key),Ke(i)&&(f=i.ref,Qe(i,a));for(d in i)R.call(i,d)&&!$e.hasOwnProperty(d)&&(s[d]=i[d]);if(e&&e.defaultProps){var b=e.defaultProps;for(d in b)s[d]===void 0&&(s[d]=b[d])}if(l||f){var c=typeof e==\"function\"?e.displayName||e.name||\"Unknown\":e;l&&Je(s,c),f&&Ze(s,c)}return en(e,l,f,a,r,U.current,s)}}var W=y.ReactCurrentOwner,he=y.ReactDebugCurrentFrame;function w(e){if(e){var i=e._owner,o=P(e.type,e._source,i?i.type:null);he.setExtraStackFrame(o)}else he.setExtraStackFrame(null)}var L;L=!1;function Y(e){return typeof e==\"object\"&&e!==null&&e.$$typeof===n}function ge(){{if(W.current){var e=_(W.current.type);if(e)return`\n\nCheck the render method of \\``+e+\"`.\"}return\"\"}}function tn(e){{if(e!==void 0){var i=e.fileName.replace(/^.*[\\\\\\/]/,\"\"),o=e.lineNumber;return`\n\nCheck your code at `+i+\":\"+o+\".\"}return\"\"}}var _e={};function on(e){{var i=ge();if(!i){var o=typeof e==\"string\"?e:e.displayName||e.name;o&&(i=`\n\nCheck the top-level render call using <`+o+\">.\")}return i}}function pe(e,i){{if(!e._store||e._store.validated||e.key!=null)return;e._store.validated=!0;var o=on(i);if(_e[o])return;_e[o]=!0;var r=\"\";e&&e._owner&&e._owner!==W.current&&(r=\" It was passed a child from \"+_(e._owner.type)+\".\"),w(e),m('Each child in a list should have a unique \"key\" prop.%s%s See https://reactjs.org/link/warning-keys for more information.',o,r),w(null)}}function Ne(e,i){{if(typeof e!=\"object\")return;if(M(e))for(var o=0;o<e.length;o++){var r=e[o];Y(r)&&pe(r,i)}else if(Y(e))e._store&&(e._store.validated=!0);else if(e){var a=Ee(e);if(typeof a==\"function\"&&a!==e.entries)for(var d=a.call(e),s;!(s=d.next()).done;)Y(s.value)&&pe(s.value,i)}}}function rn(e){{var i=e.type;if(i==null||typeof i==\"string\")return;var o;if(typeof i==\"function\")o=i.propTypes;else if(typeof i==\"object\"&&(i.$$typeof===H||i.$$typeof===k))o=i.propTypes;else return;if(o){var r=_(i);Le(o,e.props,\"prop\",r,e)}else if(i.PropTypes!==void 0&&!L){L=!0;var a=_(i);m(\"Component %s declared `PropTypes` instead of `propTypes`. Did you misspell the property assignment?\",a||\"Unknown\")}typeof i.getDefaultProps==\"function\"&&!i.getDefaultProps.isReactClassApproved&&m(\"getDefaultProps is only used on classic React.createClass definitions. Use a static property named `defaultProps` instead.\")}}function ln(e){{for(var i=Object.keys(e.props),o=0;o<i.length;o++){var r=i[o];if(r!==\"children\"&&r!==\"key\"){w(e),m(\"Invalid prop `%s` supplied to `React.Fragment`. React.Fragment can only have `key` and `children` props.\",r),w(null);break}}e.ref!==null&&(w(e),m(\"Invalid attribute `ref` supplied to `React.Fragment`.\"),w(null))}}function sn(e,i,o,r,a,d){{var s=Ie(e);if(!s){var l=\"\";(e===void 0||typeof e==\"object\"&&e!==null&&Object.keys(e).length===0)&&(l+=\" You likely forgot to export your component from the file it's defined in, or you might have mixed up default and named imports.\");var f=tn(a);f?l+=f:l+=ge();var b;e===null?b=\"null\":M(e)?b=\"array\":e!==void 0&&e.$$typeof===n?(b=\"<\"+(_(e.type)||\"Unknown\")+\" />\",l=\" Did you accidentally export a JSX literal instead of a component?\"):b=typeof e,m(\"React.jsx: type is invalid -- expected a string (for built-in components) or a class/function (for composite components) but got: %s.%s\",b,l)}var c=nn(e,i,o,a,d);if(c==null)return c;if(s){var h=i.children;if(h!==void 0)if(r)if(M(h)){for(var D=0;D<h.length;D++)Ne(h[D],e);Object.freeze&&Object.freeze(h)}else m(\"React.jsx: Static children should always be an array. You are likely explicitly calling React.jsxs or React.jsxDEV. Use the Babel transform instead.\");else Ne(h,e)}return e===v?ln(c):rn(c),c}}var an=sn;q.Fragment=v,q.jsxDEV=an})()});var Ue=V((Dn,Ge)=>{\"use strict\";Ge.exports=De()});var vn={};fn(vn,{default:()=>Nn,frontmatter:()=>_n});var t=hn(Ue()),_n={author:\"zmzlois\",date:new Date(16730496e5),title:\"Vectorising David\",type:\"Blog\",featured:!0,published:!0,description:\"Part of the original draft I have written for the book Prompt\"};function He(u){let n=Object.assign({h5:\"h5\",a:\"a\",p:\"p\",strong:\"strong\",ul:\"ul\",li:\"li\",em:\"em\",h3:\"h3\",img:\"img\",h2:\"h2\"},u.components);return(0,t.jsxDEV)(t.Fragment,{children:[(0,t.jsxDEV)(n.h5,{children:[\"This article is part of the original draft I have written for the book \",(0,t.jsxDEV)(n.a,{href:\"https://prompt.mba/\",children:\"Prompt\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:12,columnNumber:78},this),\" with my friend \",(0,t.jsxDEV)(n.a,{href:\"https://medium.com/u/85197d987a5b?source=post_page-----48253fc1582--------------------------------\",children:\"David Boyle\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:12,columnNumber:123},this),\". Touch lightly on transformer architecture and how tokeniser work.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:12,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"12 min read\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:14,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.strong,{children:\"A very brief intro in case the whole article is too long to read:\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:16,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:16,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Think about words in Bytes, a bunch of 0 and 1. If you get the impression that AI is all about probabilities\\u2013you are not wrong. ChatGPT uses a bunch of 0 and 1, encoded from words within sentences, as input and a huge vocabulary dictionary. It didn\\u2019t generate a whole block of text for you; it was \\u201Cthinking\\u201D the best next word while generating every word. In a nutshell, this is how GPT-like models output in the simplest form:\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:18,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.ul,{children:[`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This (First words and then what\\u2019s the next most possible word?)\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:20,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:20,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book (High probability as it is closer to the prompt)\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:21,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:21,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book talks (High probability a verb appears after a noun)\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:22,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:22,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book talks about\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:23,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:23,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book talks about artificial\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:24,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:24,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book talks about artificial intelligence\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:25,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:25,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book talks about artificial intelligence in\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:26,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:26,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book talks about artificial intelligence in marketing\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:27,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:27,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.li,{children:(0,t.jsxDEV)(n.em,{children:\"This book talks about artificial intelligence in marketing.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:28,columnNumber:3},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:28,columnNumber:1},this),`\n`]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:20,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Every time it generates a word, it looks for the next most possible word to pair with the previous one. OpenAI hired 40 data trainers / human labellers to generate training data and give feedback on the results. Then it iterates this process with more prompts, data, and feedback.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:30,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{children:\"Context: Beyond the pattern of a sentence\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:32,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"One of the most common natural language processing applications is at our fingertips: every time we type in something on our phone it suggests the next word.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:35,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"In earlier NLP models, say 2003, \",(0,t.jsxDEV)(n.a,{href:\"https://ecommons.cornell.edu/bitstream/handle/1813/6721/87-881.pdf;jsessionid=CE51FD6F7E2819ED24988A424EEA725E?sequence=1\",children:\"it treats words with static value\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:37,columnNumber:34},this),\"s. The context of \\u201Csouth\\u201D within \\u201CSomething went south.\\u201D and \\u201CI live in the south bank.\\u201D has no difference. People created \",(0,t.jsxDEV)(n.a,{href:\"https://arxiv.org/pdf/1706.03762.pdf\",children:\"a rule-based structure\\u2013using cues\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:37,columnNumber:315},this),\", such as negation to help NLP models perform better in linguistic tasks in 2012. It was quickly revoked because it draws technical limitations \\u2014 how big can this cue dictionary go? Based on the technical limitation we can already infer that the model won\\u2019t be able to understand context, nor achieve more complex linguistic tasks (ex: your prompt has several lines of text and include conflicting cues).\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:37,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1062/0*fHi9-KPrYyixzbvw\",alt:\"Example of false negation cues\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:39,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Source: Example of false negation cues from UCM-I: A Rule-based Syntactic Approach for Resolving the Scope of Negation\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:40,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:39,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h2,{children:(0,t.jsxDEV)(n.strong,{children:\"Context is important.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:42,columnNumber:4},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:42,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Take an example if someone posted on Twitter at 7:35 am:\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:44,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"\\u201C Thanks for coming in today, coach.\\u201D\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:46,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"It sounded like sheer appreciation. But if he also tweeted at 6:09 am saying,\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:48,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"\\u201COmg it\\u2019s snowing right now, and our class is supposed to start at 6 am! The coach is almost 10 minutes late!!! How can she just let us stand in the snow and wait? Shall we just leave like the teacher didn\\u2019t show up?!\\u201D\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:50,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Then we have an idea that he is probably being sarcastic about the coach being late. But breaking the two sentences separately, we have very little idea about what he was referring to or what it actually means. We will return to this \\u201Ccontext\\u201D problem a bit because we have another problem before going into some state-of-the-art neural network. And to be very honest, any neural network is just matrix multiplication. We tried to write them in plain English. Secondary school level maths is enough. If you are not happy, we have also linked to multiple resources so you can check them up \\u2014 as always :)\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:52,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Matrices are rows of vectors(or numbers, but here we refer to vectors because they have to represent length and position later on). And matrix multiplications are rows of vectors multiplied by rows of vectors. Vectors are objects with certain properties. It has both a magnitude and direction.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:54,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.strong,{children:\"Actually, we have a big Problem: Computers can\\u2019t understand text but numbers, so how are languages being processed?\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:56,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:56,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{children:\"Tokenizers: Translator between machine and human\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:58,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"A high-performance, high-functional tokenizer is essential in a machine-learning system focusing on NLP tasks. Because your users don\\u2019t want to wait as soon as they hit enter. In production, time is calculated in milliseconds. The longer they wait, the more likely they will leave the site with bad reviews. And that happened on a lot of generative AI applications.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:62,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Converting words into plain binary form is not enough. The objective of a tokenizer is simple: find the most meaningful, relevant way to represent texts by numbers. It splits text into words or subwords; then words are converted to ids through a look-up table. And there are multiple tokenization algorithms. The sentence \\u201CDavid decided to write a book.\\u201D can be converted to\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:64,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"[\\u201CDavid\\u201D, \\u201Cdecided\\u201D, \\u201Cto\\u201D, \\u201Cwrite\\u201D, \\u201C a\\u201D, \\u201Cbook\\u201D, \\u201C.\\u201D] to get\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:66,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"[(\\u201CDavid\\u201D,15), (\\u201Cdecided\\u201D20), (\\u201Cto\\u201D, 5), (\\u201Cwrite\\u201D,10), (\\u201C a\\u201D,3), ( \\u201Cbook\\u201D,8), ( \\u201C.\\u201D, 1)]\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:68,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"or character-based like [\\u201CD\\u201D, \\u201Ca\\u201D, \\u201Cv\\u201D,\\u2026. \\u201Co\\u201D, \\u201Ck\\u201D, \\u201C.\\u201D].\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:70,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"The former requires a huge vocabulary bank, a lot of memory and time to consume large text. Character-based tokenization is relatively easy and saves a lot of memory(less vocabulary!) and time. Still, it is harder for the model to learn the actual pattern of a language. In production and actual training, it results in serious performance loss.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:72,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.strong,{children:\"Take an example: Say you make a search engine and train the model with a character-based method, and you set a fuzzy search where that anything that matches 30% would count towards a related search result. I went on to search \\u201CChatGPT Prompt\\u201D and on top of my search results could be \\u201CProgressive Web App\\u201D and \\u201CChat with Putin\\u201D. Earlier NLP models used it, and the result was a joke.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:74,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:74,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"To improve model performance, models nowadays use a hybrid approach between word-level and character-level methods called subword tokenization. There are several of them; BERT uses \",(0,t.jsxDEV)(n.a,{href:\"https://static.googleusercontent.com/media/research.google.com/ja//pubs/archive/37842.pdf\",children:\"WordPiece\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:76,columnNumber:182},this),\" and ChatGPT uses Byte-level \",(0,t.jsxDEV)(n.a,{href:\"https://arxiv.org/pdf/1508.07909.pdf\",children:\"Byte-Pair Encoding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:76,columnNumber:313},this),\". The wording is irrelevant; think about how you refactor a function like this:\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:76,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"24x\\u2074 + 8x\\xB3 + 12x\\xB2 = y \\u2192 Refactored \\u2192 4x\\xB2(6x\\xB2+2x+3) = y\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:78,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"[\\u201CDavid found his id card on top of his wounds.\\u201D] will be broken down into\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:80,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"[\\u201CDav\\u201D, \\u201Cid\\u201D, \\u201Cf\\u201D, \\u201Cound\\u201D, \\u201Chis\\u201D, \\u201Cid\\u201D, \\u201Ccard\\u201D, \\u201Con\\u201D, \\u201Ctop\\u201D, \\u201Cof\\u201D, \\u201Chis\\u201D, \\u201Cw\\u201D, \\u201Cound\\u201D, \\u201Cs\\u201D.]\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:82,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Mathematically they are similar. Byte-level Byte-Pair Encoding is a very clever method in all the tokenization algorithms(they are usually pretrained too). It refactored the most frequent character pair and provided the most frequent pair with relevant values. It also forces the base vocabulary to be 256 with additional rules to deal with punctuation/spacing in a sentence. Within the vocabulary of 50,257, each corresponds to the \",(0,t.jsxDEV)(n.a,{href:\"https://huggingface.co/docs/transformers/tokenizer_summary#bytelevel-bpe\",children:\"256-byte base tokens, a special end-of-text token and the symbols learned with 50,000 merges\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:84,columnNumber:434},this),\". You can try to \\u201Ctokenize\\u201D something yourself at \",(0,t.jsxDEV)(n.a,{href:\"https://beta.openai.com/tokenizer\",children:\"OpenAI\\u2019s tokenizing tool\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:84,columnNumber:652},this),\".\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:84,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Now the gap between humans and machines is sorted. How shall we solve the context problem?\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:86,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{children:\"Attention is all you need: The science of context and transformer architecture\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:88,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Researchers were scratching their heads to find what could be the best solution for the next generation neural network \\u2014 a model doesn\\u2019t just do a dummy job\\u2013prompting a simple sentence and outputting something silly; something avoids \",(0,t.jsxDEV)(n.a,{href:\"https://programmathically.com/understanding-the-exploding-and-vanishing-gradients-problem/#:~:text=The%20exploding%20gradient%20problem%20describes,weights%2C%20and%20learning%20becomes%20unstable.\",children:\"vanishing or exploding gradient problems\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:90,columnNumber:235},this),\"; something can take a sequence as input and give a sequence as output; something has longer memory; a model saves computation cost and able to be trained with a relatively smaller amount of text.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:90,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Basically, they were looking for god.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:92,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.a,{href:\"https://en.wikipedia.org/wiki/Bag-of-words_model\",children:\"Bag-of-words\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:94,columnNumber:1},this),\", gone. \",(0,t.jsxDEV)(n.a,{href:\"http://d2l.ai/chapter_recurrent-neural-networks/rnn.html\",children:\"Recurrent neural networks\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:94,columnNumber:73},this),\", gone. \",(0,t.jsxDEV)(n.a,{href:\"https://d2l.ai/chapter_recurrent-modern/lstm.html\",children:\"Long Short-Term Memory\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:94,columnNumber:166},this),\", gone. For all the reasons listed above.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:94,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"To address the context problem and issues that occurred in other models, a bunch of researchers from Google Brain (some of them work in OpenAI now!) dropped a bombing article, \",(0,t.jsxDEV)(n.a,{href:\"https://arxiv.org/pdf/1706.03762.pdf\",children:\"Attention is all you need\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:96,columnNumber:177},this),\" in 2017, which is still at the forefront of any headlines of NLP models that shatters performance benchmarks. It brought a model that worked like a human brain(close to, but not enough) and accidentally changed computer vision, image generation and audio processing. But the strongest use cases were still in NLP: \",(0,t.jsxDEV)(n.a,{href:\"https://huggingface.co/docs/transformers/main/en/model_doc/openai-gpt\",children:\"GPT\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:96,columnNumber:557},this),\" and \",(0,t.jsxDEV)(n.a,{href:\"https://huggingface.co/docs/transformers/main/en/model_doc/bert\",children:\"BERT\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:96,columnNumber:638},this),\". Both have a lot of variations in handling different tasks, and the model proposed in the original paper has evolved since.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:96,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h2,{children:(0,t.jsxDEV)(n.strong,{children:\"Human vision systems inspired the attention mechanism proposed in the article.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:98,columnNumber:4},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:98,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Imagine how your brain processes when you see something. Say you walk into a restaurant during Christmas; you\\u2019d pay attention to the most stunning element, the Christmas Tree, the pretty waitress holding the menu waiting for you to take you to the seat, and then the seats scattered around the restaurant. You\\u2019d ignore insignificant details like the material of the floor, other customers in the restaurant or dimmed lights at the ceiling because they came off as irrelevant.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:100,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Same as the attention mechanism. It focuses on the most relevant parts in a whole block of text and anything relevant but ignores the insignificant details. Supporting the attention mechanism, they introduced a neural network called the Transformer which came with an \",(0,t.jsxDEV)(n.strong,{children:\"encoder-decoder architecture\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:102,columnNumber:269},this),\". In a nutshell, an encoder is the part that deals with input pre-processing, and the decoder deals with output pre-processing.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:102,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1116/0*lIpCoChAI8dW2hzj\",alt:\"Illustrated guide to transformers\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:104,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Source: Michael Phi, Illustrated Guide to transformers step by step explanation\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:105,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:104,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Like every data science project, there is a lot of preprocessing before the actual work.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:107,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h3,{children:\"Preprocessing \\u{1F92F}\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:109,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"The first step is input encoding. We have mentioned tokenizers before. It will convert \\u201CDavid\\u201D into numbers first (I am using one-hot encoding here because it makes much sense since you are aware computers work with binaries, but it is \",(0,t.jsxDEV)(n.a,{href:\"https://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#one-hot-vectors-are-a-bad-choice\",children:\"a bad choice\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:112,columnNumber:237},this),\" if you come across someone using it. ) at a vector size of 50,257.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:112,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*JNOmd5_Q0DM14PlT\",alt:\"Input encoding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:114,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Figure 29. Input encoding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:115,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:114,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"GPT\\u2019s input sequence is \",(0,t.jsxDEV)(n.a,{href:\"https://huggingface.co/docs/transformers/v4.25.1/en/model_doc/gpt2#transformers.GPT2Config.n_positions\",children:\"defaulted to 2,048 words\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:117,columnNumber:25},this),\". And yeah we do that with every word. GPT-like models generally take the input and guess the next best possible output.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:117,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*bVbzK70HIebcALus\",alt:\"Input and output sequences\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:119,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Input and Output sequences\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:120,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:119,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"And both input and output default to 2,048 words.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:122,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*hbeQKs9DlEBV_sFr\",alt:\"2,048 words\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:124,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:124,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"So we have a matrix of 2,048 x 50,257 and end up with a bunch of zeros and ones.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:126,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*ZmwOUg5yR4lY3ouy\",alt:\"Matrix encoding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:128,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Matrix encoding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:129,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:128,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Wait a minute, GPT-3 uses Byte-level BPE, so the output will look like the below before it was cut down to zero and ones.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:131,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"[11006, 3066, 284, 3551, 257]\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:133,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Make something shorter. Something not 50,257. Also this is a really short sentence, so where are the rest of the 2,043 words? They are filled in with empty values, and the encoder is told to ignore them (a process called \",(0,t.jsxDEV)(n.strong,{children:\"masking\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:135,columnNumber:222},this),\"; otherwise the empty value will influence the whole sentence and the model can\\u2019t pay attention to important things).\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:135,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"But there are still many vectors to deal with for longer sentences, and we know they force the base vocabulary to be zero and ones. So there are a lot of empty spaces and wasted storage.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:137,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"To solve this, we use an \",(0,t.jsxDEV)(n.strong,{children:\"embedding function\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:139,columnNumber:26},this),\": a function takes the long vector of zeros and ones and then outputs an n-length vector of numbers like so. (\",(0,t.jsxDEV)(n.a,{href:\"http://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#word-embedding-word2vec\",children:\"Word Embedding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:139,columnNumber:158},this),\", the process of mapping words to real vectors)\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:139,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:692/0*Lhb25rqsunk0DDyA\",alt:\"Embedding function in action\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:141,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"An embedding function in action\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:142,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:141,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"And then store them in a smaller dimension space (if the vector length is 2, it will be stored in a 2-dimensional space like x-y axis ones)\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:144,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:940/0*2rczZuh9wYM2-qiY\",alt:\"Words in a dimension space\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:146,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Words in a dimension space\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:147,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:146,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"But yeah, GPT\\u2019s dimension is much larger than two after multiplication between matrices: 12288 dimensions. ( A lot larger than the original paper which only has 512)\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:149,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"The first matrix below is a 2,048 x 50,257 sequence-encoding matrix.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:151,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"The second matrix is a 50,257 x 122,88 embedding-weight matrix(learnt by model).\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:153,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*i9a-QMGxmOX7Xg0T\",alt:\"Matrix multiplication\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:155,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Figure 35. Matrix multiplication\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:156,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:155,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.a,{href:\"https://www.mathsisfun.com/algebra/matrix-multiplying.html\",children:\"Multiply both of them\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:158,columnNumber:1},this),\" we get a 2,048 x 12,288 sequence-embedding matrix, and every word is represented by numbers on a \\u201Clook-up table\\u201D for the model to look at. Awesome, everything is represented by numbers now. But so far we have no information on the absolute or relative position or the words(or token if you might call it).\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:158,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"So we do some \",(0,t.jsxDEV)(n.a,{href:\"https://d2l.ai/chapter_attention-mechanisms-and-transformers/self-attention-and-positional-encoding.html#positional-encoding\",children:(0,t.jsxDEV)(n.strong,{children:\"positional encoding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:160,columnNumber:16},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:160,columnNumber:15},this),\" to give every word a position id to notify the model of each word\\u2019s position in the text before going into the multi-head attention layer (check the link if you are interested in the related literature, maths and python code).\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:160,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.h2,{children:\"Encoder and Decoder\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:162,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1116/0*z8iroxIM5cQJYz3V\",alt:\"Encoder and decoder\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:165,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Encoder and Decoder\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:166,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:165,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Oh geez, that was a hell lot of preprocessing; it\\u2019s time to get some exciting work done. You can crunch on this baby graph or go back to 4 pages before and look at the bigger transformer architecture picture. \",(0,t.jsxDEV)(n.a,{href:\"https://jalammar.github.io/illustrated-transformer/\",children:\"What do they do\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:168,columnNumber:210},this),\"? The encoder on the left maps an input sequence of text representations to a sequence of continuous representations.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:168,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"The decoder on the right generated an output sequence of symbols one element at a time. At each step the model is auto-regressive and consumes the previously generated text as additional input when generating the next. Like what we said before (Imagine the turtle in Zootopia spilling out words one by one, but much faster)And the transformer allows this overall architecture using \",(0,t.jsxDEV)(n.strong,{children:\"stacked self-attention\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:170,columnNumber:383},this),\", \",(0,t.jsxDEV)(n.strong,{children:\"point-wise, fully connected layers\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:170,columnNumber:411},this),\" to both the encoder and decoder. To clear some terms in case you get confused.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:170,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.a,{href:\"https://medium.com/@geetkal67/attention-networks-a-simple-way-to-understand-self-attention-f5fb363c736d\",children:(0,t.jsxDEV)(n.strong,{children:\"Stacked self-attention\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:172,columnNumber:2},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:172,columnNumber:1},this),\": A self-attention module takes x input and x output, is the mechanism to allow the input to interact with each other(\\u201Cself\\u201D) and find out who they should pay more attention to (\\u201Cattention\\u201D) and aggregates these interactions and attention scores as output. Stacked simply means: multiple times/layers.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:172,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"To clear out how this works. Consider a sentence: David decided to write a book about ChatGPT because it is revolutionary.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:174,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*up1JcpKkKny8s1NW\",alt:\"Attention head\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:176,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Attention head\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:177,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:176,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Transformer doubled down the idea of attention and packed together individual attention elements known as the \\u201C\",(0,t.jsxDEV)(n.strong,{children:\"attention head\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:179,columnNumber:112},this),\"\\u201D to \\u201Cfocus\\u201D on a word. It can tell the model how relevant or important that word is to understand the current word being parsed. Like the figure above we \\u201Cfocused\\u201D on [\\u201Cwrite\\u201D] and its relationship with others.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:179,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"The model learns three linear projections when the text goes through it(some \",(0,t.jsxDEV)(n.a,{href:\"https://mathworld.wolfram.com/Trigonometry.html#:~:text=The%20study%20of%20angles%20and,secant%20%2C%20sine%20%2C%20and%20tangent%20.\",children:\"trigonometry functions\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:181,columnNumber:78},this),\"): \\u201CQueries(denoted as Q)\\u201D, \\u201CKeys(denoted as K)\\u201D and \\u201CValues(denoted as V)\\u201D and GPT-3 repeat this process for 96 times to create multi-head attention, each with a different learnt query, key, value projection weights.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:181,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*z3U4zDbPZj0HLaS6\",alt:\"Attention function\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:183,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.em,{children:\"Attention function\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:184,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:183,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"More attention head(\",(0,t.jsxDEV)(n.strong,{children:\"multi-head attention\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:186,columnNumber:21},this),\") in one layer means the model can look back or forward in long text.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:186,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Our sample text was too short to have a beautiful graph so I grabbed this to help you see how beautiful it is with multi-head attention:\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:188,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*MGq43AQvoSo1U8M8\",alt:\"multi-head\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:190,columnNumber:1},this),`\nSource: `,(0,t.jsxDEV)(n.a,{href:\"https://sh-tsang.medium.com/review-attention-is-all-you-need-transformer-96c787ecdec1\",children:\"Review \\u2014 Attention is all you need\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:191,columnNumber:9},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:190,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"And the results of each attention head are concatenated together yielding a 2048x12288 matrix, which is then multiplied with a linear projection (which doesn\\u2019t change the matrix shape).\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:193,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/0*OAIle3NBiptXTcb5\",alt:\"Ensuring matrix shape doesn't change\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:195,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:195,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[(0,t.jsxDEV)(n.a,{href:\"https://paperswithcode.com/method/position-wise-feed-forward-layer\",children:(0,t.jsxDEV)(n.strong,{children:\"Point-wise fully connected layer\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:197,columnNumber:2},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:197,columnNumber:1},this),\": Means all connections between neurons(they are just numbers, in this case, there are several ones behind the dot like 0.00000012) have a fixed weight of 1(they all add up to 1) which means the output of each neuron in the previous layer is passed through to the next layer without any modification. This method allows the model to improve performance(without neglecting words).\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:197,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"And more layers of attention mean that your model can then learn a higher level of both syntactic structures and potentially, semantic meaning. The table below shows a dummy example of what the output for the word \\u201Cwrite\\u201D from the attention layer might look like by multiplying different weight matrices together and each of them obtaining a weight to try and identify which words in the sentence the network should \\u201Cpay attention\\u201D if they are important in the context.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:199,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:(0,t.jsxDEV)(n.img,{src:\"https://miro.medium.com/v2/resize:fit:1400/format:webp/1*w_SLvMH3wuKO-iuRdenJpw.png\",alt:\"Weights of a sentence\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:201,columnNumber:1},this)},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:201,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Finally we tackled the context problem. BERT-like models and GPT-like models both use the attention mechanism of the transformer architecture to learn context from text.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:203,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"And by learning context, potentially, these models can develop some level of language skill (how they function is closer to the human brain after all! Note that I used \\u201Ccloser\\u201D, not \\u201Cclose\\u201D. We still have a long way to go until we reach a place where models I\\u2019m work like the brain.) which enables them to perform better on a range of language tasks(answering questions, summarising etc.).\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:205,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"After every output we get a matrix of 2,048x12,288! Huge! Now we just need to reverse engineer the \",(0,t.jsxDEV)(n.a,{href:\"http://d2l.ai/chapter_natural-language-processing-pretraining/word2vec.html#word-embedding-word2vec\",children:\"work embedding\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:207,columnNumber:100},this),\" process and map the data back to text. After all, we spent a hell lot of time learning it!! Also GPT uses the parameter top-k to limit the output, so it always picks the most likely words to make it grammatically correct \\u2014 seeing something not true? Don\\u2019t get surprised.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:207,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:[\"Another thing: GPT-3, the base model of ChatGPT, uses \",(0,t.jsxDEV)(n.a,{href:\"https://openai.com/blog/sparse-transformer/\",children:\"sparse attention\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:209,columnNumber:55},this),\", allowing more efficient computation. So you see, these guys designed the whole architecture, everything with speed in mind.\"]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:209,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"That was long \\u{1FAE0} And I still have a load in reinforcement learning with human feedback left in my draft.\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:211,columnNumber:1},this),`\n`,(0,t.jsxDEV)(n.p,{children:\"Until next time \\u{1FAE1}\"},void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:213,columnNumber:1},this)]},void 0,!0,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\",lineNumber:1,columnNumber:1},this)}function pn(u={}){let{wrapper:n}=u.components||{};return n?(0,t.jsxDEV)(n,Object.assign({},u,{children:(0,t.jsxDEV)(He,u,void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\"},this)}),void 0,!1,{fileName:\"/Users/lois/Documents/GitHub/lois-blog-v5/content/blogs/_mdx_bundler_entry_point-3603c705-6abd-4a0f-b2bd-9512c0b0e15c.mdx\"},this):He(u)}var Nn=pn;return gn(vn);})();\n/*! Bundled license information:\n\nreact/cjs/react-jsx-dev-runtime.development.js:\n  (**\n   * @license React\n   * react-jsx-dev-runtime.development.js\n   *\n   * Copyright (c) Facebook, Inc. and its affiliates.\n   *\n   * This source code is licensed under the MIT license found in the\n   * LICENSE file in the root directory of this source tree.\n   *)\n*/\n;return Component;"
  },
  "_id": "blogs/vectorising-david.mdx",
  "_raw": {
    "sourceFilePath": "blogs/vectorising-david.mdx",
    "sourceFileName": "vectorising-david.mdx",
    "sourceFileDir": "blogs",
    "contentType": "mdx",
    "flattenedPath": "blogs/vectorising-david"
  },
  "type": "Blog",
  "url": "/blogs/vectorising-david",
  "slug": "vectorising-david"
}